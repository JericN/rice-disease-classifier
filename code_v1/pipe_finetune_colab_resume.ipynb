{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup and Library Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Installing Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jO63oYRzL5Sv",
    "outputId": "c8d72843-15dd-457a-8c99-f80c46754c00"
   },
   "outputs": [],
   "source": [
    "! pip install --quiet \"transformers[torch]\"\n",
    "! pip install --quiet evaluate\n",
    "! pip install --quiet tabulate\n",
    "! pip install --quiet ipywidgets\n",
    "! pip install --quiet datasets\n",
    "! pip install --quiet pillow\n",
    "! pip install --quiet scikit-learn\n",
    "! pip install --quiet tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch for tensor operations\n",
    "import torch\n",
    "\n",
    "# Hugging Face libraries for training and transformer models\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "# Evaluation metrics and utilities\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Loading datasets for training and evaluation\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Data manipulation and display utilities\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logging into Hugging Face Hub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Execute the login function to access the Hugging Face account\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Connect to google drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining Model, Dataset Paths, and Output Directories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths for saving model training outputs and logs, incorporating model and dataset names along with the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and dataset paths\n",
    "model_path = \"\"\n",
    "dataset_path = \"cvmil/rice-disease-\"\n",
    "train_epochs = 15\n",
    "\n",
    "# Create a timestamped output directory for saving training results\n",
    "# dt_string = datetime.now().strftime(\"%m%d%y\")\n",
    "dt_string = \"111824\"\n",
    "output_dir = f\"./drive/Shareddrives/CS198-Drones/training_output/{model_path.split('/')[-1]}_{dataset_path.split('/')[-1]}_{dt_string}\"\n",
    "\n",
    "# Define directory for storing training logs\n",
    "logging_dir = f\"{output_dir}/logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation and Processing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section handles the dataset loading, label extraction, image processing setup, and defines necessary functions for data transformation, batching, and metric computation to prepare the data for model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Dataset and Extract Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from huggingface and extract the class labels from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "# Extract class labels from the training set\n",
    "labels = dataset['train'].features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and display a table showing class distribution across training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dataset['train'].features['label'].int2str\n",
    "\n",
    "# Count the number of samples per class in each split\n",
    "train_counts = Counter(dataset['train']['label'])\n",
    "validation_counts = Counter(dataset['validation']['label'])\n",
    "\n",
    "# Create a DataFrame for the class distribution\n",
    "data = {\n",
    "    'ID': list(range(len(labels))),\n",
    "    'Label': labels,\n",
    "    'Training': [train_counts[i] if i in train_counts else 0 for i in range(len(labels))],\n",
    "    'Validation': [validation_counts[i] if i in validation_counts else 0 for i in range(len(labels))],\n",
    "}\n",
    "\n",
    "# Display the class distribution in a table format\n",
    "df = pd.DataFrame(data)\n",
    "print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize Image Processor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and initialize the image processor from the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image processor from the pre-trained model\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "print(processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation and Processing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mappings for label-to-ID and ID-to-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {c: idx for idx, c in enumerate(labels)}\n",
    "id2label = {idx: c for idx, c in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the transformation function to process the image batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(batch):\n",
    "    batch['image'] = [x.convert('RGB') for x in batch['image']]\n",
    "    inputs = processor(batch['image'], return_tensors='pt')\n",
    "    inputs['labels'] = batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the custom collation function for batching pixel values and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to compute accuracy during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Apply Data Transformations to Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the defined transformation function to the dataset for preprocessing. </br>\n",
    "Note: This assumes that data augmentation and normalization have already been handled in the previous pipeline and is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Initialization and Trainer Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section handles the initialization of the model, configuration of training parameters, and setting up the Trainer for fine-tuning, including the datasets, data processing, and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize Pre-trained Model for Fine-tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pre-trained image classification model, configuring it with the correct label mappings and number of labels for the fine-tuning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and configure it for fine-tuning\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_path,                  # Path to the pre-trained model\n",
    "    num_labels=len(labels),      # Set the number of labels for classification\n",
    "    id2label=id2label,           # Map from ID to label\n",
    "    label2id=label2id,           # Map from label to ID\n",
    "    ignore_mismatched_sizes=True # Ignore size mismatches in weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Freeze Model Parameters for Fine-tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the model parameters except for those related to the classifier layers, allowing fine-tuning only on the specified layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = name.startswith(('classifier', 'distillation_classifier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many parameters are there in the model along with how many are actually going to be trained now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {num_params:,} | Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Training Arguments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the training configuration with parameters such as batch size, number of epochs, learning rate, and logging strategies for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=64,          # Batch size for training\n",
    "    per_device_eval_batch_size=64,           # Batch size for evaluation\n",
    "    eval_strategy=\"epoch\",                   # Evaluation strategy\n",
    "    save_strategy=\"epoch\",                   # Save strategy\n",
    "    logging_strategy=\"epoch\",                # Logging strategy\n",
    "    num_train_epochs=train_epochs,           # Number of training epochs\n",
    "    learning_rate=3e-4,                      # Learning rate for training\n",
    "    warmup_ratio=0.1,                        # Warmup ratio for learning rate scheduler\n",
    "    save_total_limit=5,                      # Limit the number of saved models\n",
    "    remove_unused_columns=False,             # Retain unused columns in the dataset\n",
    "    load_best_model_at_end=True,             # Load best model at the end of training\n",
    "    report_to=['tensorboard'],               # Log to TensorBoard\n",
    "    logging_dir=logging_dir,                 # Directory for logging\n",
    "    output_dir=output_dir,                   # Directory for saving outputs\n",
    "    push_to_hub=True,                        # Push model to Hugging Face Hub\n",
    "    resume_from_checkpoint=True,             # Resume from checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Trainer object with the model, training arguments, data collator, metrics computation, and datasets for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"],\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start Fine-tuning Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiates the fine-tuning of the model using the Trainer, applying the specified training configurations, such as the batch size, learning rate, and number of epochs. During training, the model will be evaluated at the end of each epoch on the validation dataset using the compute_metrics function, which calculates accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will undergo the following process during fine-tuning:\n",
    "- **Training**: The model will be trained on the training dataset for the specified number of epochs.\n",
    "- **Evaluation**: After each epoch, the model will be evaluated on the validation dataset, and accuracy will be computed using the compute_metrics function.\n",
    "- **Metrics Logging**: The training progress and evaluation results will be logged to TensorBoard and can be monitored during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Model and Training State**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training process, the model and relevant training state are saved. This includes saving the model weights, training metrics, and the state of the trainer, ensuring that training progress can be restored if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "trainer.save_model()\n",
    "\n",
    "# Log and save training metrics for later reference\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "\n",
    "# Save the state of the trainer, including configuration and optimizer state\n",
    "trainer.save_state()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
