{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JericN/rice-disease-classifier/blob/main/results_inference_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive"
      ],
      "metadata": {
        "id": "_pRaFfv-vSYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r5WKr3l9vMoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "u9STWo75vUWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --quiet -U fvcore\n",
        "! pip install --quiet tqdm"
      ],
      "metadata": {
        "id": "UUKGlOKk7JLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "LCqxdCg_vWtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForImageClassification\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "MYZ8meYG8kr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Test Config"
      ],
      "metadata": {
        "id": "LCSN_Cg2vaCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output path in Google Drive\n",
        "root_dir = \"/content/drive/Shareddrives/CS198-Drones/\"\n",
        "output_path = '[TESTv5] Results/model_benchmark.xlsx'\n",
        "\n",
        "# Define the list of model names or paths from Hugging Face\n",
        "model_names = [\n",
        "    \"SodaXII/mobilevit-small_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"SodaXII/vit-base-patch16-224_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"SodaXII/mobilevitv2_150.cvnets_in22k_ft_in1k_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "]\n",
        "\n",
        "# Define batch size and number of iterations\n",
        "batch_size = 64\n",
        "num_iterations = 100"
      ],
      "metadata": {
        "id": "cbzbLYyQ7nnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Model Inference Data"
      ],
      "metadata": {
        "id": "eTZCPo9OvcYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model_name, batch_size=64, num_iterations=100):\n",
        "    try:\n",
        "        # Initialize model and move to GPU\n",
        "        model = AutoModelForImageClassification.from_pretrained(model_name).to(\"cuda\")\n",
        "        model.eval()\n",
        "\n",
        "        # Create a dummy input tensor with the specified batch size and move it to GPU\n",
        "        input_tensor = torch.randn(batch_size, 3, 224, 224).to(\"cuda\")\n",
        "\n",
        "        # Warm-up iterations\n",
        "        for _ in range(20):\n",
        "            with torch.no_grad():\n",
        "                _ = model(input_tensor)\n",
        "\n",
        "        # Reset peak memory statistics before inference\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        # Create CUDA events for timing\n",
        "        start_event = torch.cuda.Event(enable_timing=True)\n",
        "        end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        # Record the start event\n",
        "        start_event.record()\n",
        "\n",
        "        # Run inference multiple times\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_iterations):\n",
        "                _ = model(input_tensor)\n",
        "\n",
        "        # Record the end event\n",
        "        end_event.record()\n",
        "\n",
        "        # Wait for the events to be recorded\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Calculate average inference time per batch in milliseconds\n",
        "        total_time_ms = start_event.elapsed_time(end_event)\n",
        "        average_inference_time_ms = total_time_ms / num_iterations\n",
        "\n",
        "        # Retrieve peak memory usage\n",
        "        peak_memory = torch.cuda.max_memory_allocated()\n",
        "\n",
        "        # Perform a forward pass to collect FLOP counts\n",
        "        flop_counts = FlopCountAnalysis(model, input_tensor)\n",
        "        total_flops = flop_counts.total()\n",
        "\n",
        "        # Return the evaluation metrics\n",
        "        return {\n",
        "            \"Model\": model_name,\n",
        "            \"FLOPs\": total_flops,\n",
        "            \"Average Inference Time (ms)\": average_inference_time_ms,\n",
        "            \"Peak GPU Memory Usage (MB)\": peak_memory / (1024 ** 2)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while evaluating model {model_name}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "MPkPLDWLtwUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runner Function"
      ],
      "metadata": {
        "id": "wfoZoZ7nvgIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = os.path.join(root_dir, output_path)\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Iterate over each model and evaluate\n",
        "for model_name in tqdm(model_names, desc=\"Evaluating Models\"):\n",
        "    metrics = evaluate_model(model_name, batch_size, num_iterations)\n",
        "    if metrics:\n",
        "        results.append(metrics)\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "df_models = pd.DataFrame(results)\n",
        "\n",
        "# Save the results to an Excel file in Google Drive\n",
        "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "    df_models.to_excel(writer, sheet_name=\"Model Info\", index=False)\n",
        "\n",
        "print(f\"Benchmark results saved to: {output_file}\")"
      ],
      "metadata": {
        "id": "pNUbT7mNt_5i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}