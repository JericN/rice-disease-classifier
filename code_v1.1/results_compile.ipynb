{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_3v2vgjVomD"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow4zNwDaVomG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtjKCeSFVomH"
      },
      "source": [
        "### Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEn56mT8PgHn"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s0_6QDo3yj_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiSNFNNSVomI"
      },
      "source": [
        "### Initialize Test Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekE91V1G35R2"
      },
      "outputs": [],
      "source": [
        "output_dir = \"/content/drive/Shareddrives/CS198-Drones/[v5] Results\"\n",
        "\n",
        "evaluation_dir = \"/content/drive/Shareddrives/CS198-Drones/[v5] Model Evaluation/\"\n",
        "results_output_file = f\"{output_dir}/model_evaluation.xlsx\"\n",
        "\n",
        "training_dir = \"/content/drive/Shareddrives/CS198-Drones/[v5] Training Output/\"\n",
        "training_output_file = f\"{output_dir}/training_metrics.xlsx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBi0NvtFVomJ"
      },
      "source": [
        "### Compile Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models in desired order\n",
        "model_order = [\n",
        "    \"convnextv2-base-1k-224_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"vit-hybrid-base-bit-384_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"vit-base-patch16-224_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"swin-base-patch4-window7-224_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"deit-base-patch16-224_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"dinov2-base_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "\n",
        "    \"vit_small_patch16_224.augreg_in21k_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"swin-tiny-patch4-window7-224_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"deit-small-patch16-224_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"convnextv2-tiny-1k-224_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "\n",
        "    \"mobilevit-small_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"mobilevitv2_150.cvnets_in22k_ft_in1k_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"efficientnet-b2_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"efficientvit_b1.r224_in1k_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"efficientvit_m4.r224_in1k_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"efficientformer_l1.snap_dist_in1k_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "    \"efficientformerv2_s2.snap_dist_in1k_rice-leaf-disease-augmented-v4_v5_fft\",\n",
        "]\n",
        "\n",
        "model_name = [\n",
        "    \"ConvNeXtV2 Base\",\n",
        "    \"ViT Hybrid Base\",\n",
        "    \"ViT Base\",\n",
        "    \"Swin Base\",\n",
        "    \"DeiT Base\",\n",
        "    \"DINOv2 Base\",\n",
        "\n",
        "    \"ViT Small\",\n",
        "    \"Swin Small\",\n",
        "    \"DeiT Small\",\n",
        "    \"ConvNeXtV2 Small\",\n",
        "\n",
        "    \"MobileViT Small\",\n",
        "    \"MobileViTv2 150\",\n",
        "    \"EfficientNet B2\",\n",
        "    \"EfficientViT B1\",\n",
        "    \"EfficientViT M4\",\n",
        "    \"EfficientFormer L1\",\n",
        "    \"EfficientFormerV2 S2\",\n",
        "]\n",
        "\n",
        "rename_map = dict(zip(model_order, model_name))"
      ],
      "metadata": {
        "id": "HFJAun3aZMxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j29T3BEaVomJ"
      },
      "outputs": [],
      "source": [
        "def compile_evaluation_metrics(json_folder, output_file):\n",
        "    print(f\"\\n⌛️ Compiling evaluation metrics from {json_folder}...\")\n",
        "    precision_data = []\n",
        "    recall_data = []\n",
        "    f1_data = []\n",
        "    overall_data = []\n",
        "\n",
        "    for model_dir in os.listdir(json_folder):\n",
        "        print(f\"\\n⌛️ Processing {model_dir}...\")\n",
        "        model_path = os.path.join(json_folder, model_dir)\n",
        "        json_file_path = os.path.join(model_path, \"report.json\")\n",
        "        if os.path.isdir(model_path) and os.path.isfile(json_file_path):\n",
        "            with open(json_file_path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                model_name = model_dir\n",
        "\n",
        "                for class_name, metrics in data.items():\n",
        "                    if isinstance(metrics, dict):\n",
        "                        precision_data.append({\n",
        "                            \"Model\": model_name,\n",
        "                            \"Class\": class_name,\n",
        "                            \"Precision\": metrics.get(\"precision\", None),\n",
        "                        })\n",
        "                        recall_data.append({\n",
        "                            \"Model\": model_name,\n",
        "                            \"Class\": class_name,\n",
        "                            \"Recall\": metrics.get(\"recall\", None),\n",
        "                        })\n",
        "                        f1_data.append({\n",
        "                            \"Model\": model_name,\n",
        "                            \"Class\": class_name,\n",
        "                            \"F1-Score\": metrics.get(\"f1-score\", None),\n",
        "                        })\n",
        "\n",
        "                # Add overall data\n",
        "                weighted_ave = data[\"weighted avg\"]\n",
        "                overall_data.append({\n",
        "                    \"Model\": model_name,\n",
        "                    \"Accuracy\": data.get(\"accuracy\", None),\n",
        "                    \"Precision\": weighted_ave.get(\"precision\", None),\n",
        "                    \"Recall\": weighted_ave.get(\"recall\", None),\n",
        "                    \"F1-Score\": weighted_ave.get(\"f1-score\", None),\n",
        "                    \"Support\": weighted_ave.get(\"support\", None),\n",
        "                    \"Eval Time\": data.get(\"evaluation_time_sec\", None),\n",
        "                })\n",
        "\n",
        "    print(\"\\n⌛️ Saving evaluation metrics to Excel file...\")\n",
        "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
        "        p_df = pd.DataFrame(precision_data).pivot(\n",
        "            index=\"Model\", columns=\"Class\", values=\"Precision\"\n",
        "        )\n",
        "\n",
        "        p_df = p_df.reindex(model_order)\n",
        "        p_df.to_excel(writer, sheet_name=\"Precision\")\n",
        "\n",
        "        r_df = pd.DataFrame(recall_data).pivot(\n",
        "            index=\"Model\", columns=\"Class\", values=\"Recall\"\n",
        "        )\n",
        "        r_df = r_df.reindex(model_order)\n",
        "        r_df.to_excel(writer, sheet_name=\"Recall\")\n",
        "\n",
        "        f1_df = pd.DataFrame(f1_data).pivot(\n",
        "            index=\"Model\", columns=\"Class\", values=\"F1-Score\"\n",
        "        )\n",
        "        f1_df = f1_df.reindex(model_order)\n",
        "        f1_df.to_excel(writer, sheet_name=\"F1-Score\")\n",
        "\n",
        "        pd.DataFrame(overall_data).to_excel(\n",
        "            writer, sheet_name=\"Overall Accuracy\", index=False\n",
        "        )\n",
        "\n",
        "    print(f\"\\n✅ Evaluation metrics compiled and saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTCouPpKVomJ"
      },
      "source": [
        "### Compile Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_QF1Cvb7rfJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def compile_training_metrics(metrics_folder, output_file):\n",
        "    print(f\"\\n⌛️ Compiling training metrics from {metrics_folder}...\")\n",
        "\n",
        "    # Define metric categories (these will be used as sheet names)\n",
        "    metric_names = [\n",
        "        \"loss\", \"grad_norm\", \"learning_rate\", \"eval_loss\", \"eval_accuracy\",\n",
        "        \"eval_runtime\", \"eval_samples_per_second\", \"eval_steps_per_second\",\n",
        "        \"gpu_vram_allocated_mb\", \"gpu_vram_reserved_mb\", \"timestamp\"\n",
        "    ]\n",
        "\n",
        "    # Initialize a dictionary to hold DataFrames for each metric\n",
        "    metrics_dict = {metric: pd.DataFrame() for metric in metric_names}\n",
        "\n",
        "    # Process only models in the defined list, in order\n",
        "    for model_dir in model_order:\n",
        "        print(f\"\\n⌛️ Processing {model_dir}...\")\n",
        "        model_path = os.path.join(metrics_folder, model_dir)\n",
        "        metrics_file = os.path.join(model_path, \"training_metrics.xlsx\")\n",
        "\n",
        "        if os.path.isdir(model_path) and os.path.isfile(metrics_file):\n",
        "            df = pd.read_excel(metrics_file)\n",
        "\n",
        "            if \"epoch\" not in df.columns:\n",
        "                print(f\"Skipping {model_dir} (No 'epoch' column found)\")\n",
        "                continue\n",
        "\n",
        "            # Add each metric to its respective dictionary entry\n",
        "            for metric in metric_names:\n",
        "                if metric in df.columns:\n",
        "                    if metrics_dict[metric].empty:\n",
        "                        metrics_dict[metric] = df[[\"epoch\", metric]].copy()\n",
        "                        metrics_dict[metric].rename(columns={metric: model_dir}, inplace=True)\n",
        "                    else:\n",
        "                        metrics_dict[metric] = pd.merge(\n",
        "                            metrics_dict[metric],\n",
        "                            df[[\"epoch\", metric]].rename(columns={metric: model_dir}),\n",
        "                            on=\"epoch\",\n",
        "                            how=\"outer\"\n",
        "                        )\n",
        "\n",
        "    # Save to Excel\n",
        "    print(f\"\\n⌛️ Saving compiled metrics to {output_file}...\")\n",
        "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
        "        for metric, df in metrics_dict.items():\n",
        "            if not df.empty:\n",
        "                # Reorder columns (after epoch) to match the models list\n",
        "                cols = ['epoch'] + [m for m in model_order if m in df.columns]\n",
        "                df = df[cols]\n",
        "                df = df.rename(columns=rename_map)\n",
        "                df.to_excel(writer, sheet_name=metric, index=False)\n",
        "\n",
        "    print(f\"\\n✅ Training metrics compiled and saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-NZBiDvVomK"
      },
      "source": [
        "### Runner Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALLdEOUEVomK"
      },
      "outputs": [],
      "source": [
        "compile_evaluation_metrics(evaluation_dir, results_output_file)\n",
        "compile_training_metrics(training_dir, training_output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}