{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NEn56mT8PgHn"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1s0_6QDo3yj_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Test Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ekE91V1G35R2"
      },
      "outputs": [],
      "source": [
        "output_dir = \"/content/drive/Shareddrives/CS198-Drones/[v4] Results\"\n",
        "\n",
        "evaluation_dir = \"/content/drive/Shareddrives/CS198-Drones/[v4] Model Evaluation/\"\n",
        "results_output_file = f\"{output_dir}model_evaluation.xlsx\"\n",
        "\n",
        "training_dir = \"/content/drive/Shareddrives/CS198-Drones/[v4] Training Output/\"\n",
        "training_output_file = f\"{output_dir}training_metrics.xlsx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compile_evaluation_metrics(json_folder, output_file):\n",
        "    print(f\"\\n⌛️ Compiling evaluation metrics from {json_folder}...\")\n",
        "    precision_data = []\n",
        "    recall_data = []\n",
        "    f1_data = []\n",
        "    overall_data = []\n",
        "\n",
        "    for model_dir in os.listdir(json_folder):\n",
        "        print(f\"\\n⌛️ Processing {model_dir}...\")\n",
        "        model_path = os.path.join(json_folder, model_dir)\n",
        "        json_file_path = os.path.join(model_path, \"report.json\")\n",
        "        if os.path.isdir(model_path) and os.path.isfile(json_file_path):\n",
        "            with open(json_file_path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                model_name = model_dir\n",
        "\n",
        "                for class_name, metrics in data.items():\n",
        "                    if isinstance(metrics, dict):\n",
        "                        precision_data.append({\n",
        "                            \"Model\": model_name,\n",
        "                            \"Class\": class_name,\n",
        "                            \"Precision\": metrics.get(\"precision\", None),\n",
        "                        })\n",
        "                        recall_data.append({\n",
        "                            \"Model\": model_name,\n",
        "                            \"Class\": class_name,\n",
        "                            \"Recall\": metrics.get(\"recall\", None),\n",
        "                        })\n",
        "                        f1_data.append({\n",
        "                            \"Model\": model_name,\n",
        "                            \"Class\": class_name,\n",
        "                            \"F1-Score\": metrics.get(\"f1-score\", None),\n",
        "                        })\n",
        "\n",
        "                # Add overall data\n",
        "                weighted_ave = data[\"weighted avg\"]\n",
        "                overall_data.append({\n",
        "                    \"Model\": model_name,\n",
        "                    \"Accuracy\": data.get(\"accuracy\", None),\n",
        "                    \"Precision\": weighted_ave.get(\"precision\", None),\n",
        "                    \"Recall\": weighted_ave.get(\"recall\", None),\n",
        "                    \"F1-Score\": weighted_ave.get(\"f1-score\", None),\n",
        "                    \"Support\": weighted_ave.get(\"support\", None),\n",
        "                    \"Eval Time\": data.get(\"evaluation_time_sec\", None),\n",
        "                })\n",
        "\n",
        "    print(\"\\n⌛️ Saving evaluation metrics to Excel file...\")\n",
        "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
        "        pd.DataFrame(precision_data).pivot(\n",
        "            index=\"Model\", columns=\"Class\", values=\"Precision\"\n",
        "        ).to_excel(writer, sheet_name=\"Precision\")\n",
        "\n",
        "        pd.DataFrame(recall_data).pivot(\n",
        "            index=\"Model\", columns=\"Class\", values=\"Recall\"\n",
        "        ).to_excel(writer, sheet_name=\"Recall\")\n",
        "\n",
        "        pd.DataFrame(f1_data).pivot(\n",
        "            index=\"Model\", columns=\"Class\", values=\"F1-Score\"\n",
        "        ).to_excel(writer, sheet_name=\"F1-Score\")\n",
        "\n",
        "        pd.DataFrame(overall_data).to_excel(\n",
        "            writer, sheet_name=\"Overall Accuracy\", index=False\n",
        "        )\n",
        "\n",
        "    print(f\"\\n✅ Evaluation metrics compiled and saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y_QF1Cvb7rfJ"
      },
      "outputs": [],
      "source": [
        "def compile_training_metrics(metrics_folder, output_file):\n",
        "    print(f\"\\n⌛️ Compiling training metrics from {metrics_folder}...\")\n",
        "    # Define metric categories (these will be used as sheet names)\n",
        "    metric_names = [\n",
        "        \"loss\", \"grad_norm\", \"learning_rate\", \"eval_loss\", \"eval_accuracy\",\n",
        "        \"eval_runtime\", \"eval_samples_per_second\", \"eval_steps_per_second\",\n",
        "        \"gpu_vram_allocated_mb\", \"gpu_vram_reserved_mb\", \"timestamp\"\n",
        "    ]\n",
        "\n",
        "    # Initialize a dictionary to hold DataFrames for each metric\n",
        "    metrics_dict = {metric: pd.DataFrame() for metric in metric_names}\n",
        "\n",
        "    # Loop through model folders\n",
        "    for model_dir in os.listdir(metrics_folder):\n",
        "        print(f\"\\n⌛️ Processing {model_dir}...\")\n",
        "        model_path = os.path.join(metrics_folder, model_dir)\n",
        "        metrics_file = os.path.join(model_path, \"training_metrics.xlsx\")\n",
        "\n",
        "        if os.path.isdir(model_path) and os.path.isfile(metrics_file):\n",
        "            df = pd.read_excel(metrics_file)\n",
        "\n",
        "            if \"epoch\" not in df.columns:\n",
        "                print(f\"Skipping {model_dir} (No 'epoch' column found)\")\n",
        "                continue\n",
        "\n",
        "            # Add each metric to its respective dictionary entry\n",
        "            for metric in metric_names:\n",
        "                if metric in df.columns:\n",
        "                    if metrics_dict[metric].empty:\n",
        "                        metrics_dict[metric] = df[[\"epoch\", metric]].copy()\n",
        "                        metrics_dict[metric].rename(columns={metric: model_dir}, inplace=True)\n",
        "                    else:\n",
        "                        metrics_dict[metric] = pd.merge(metrics_dict[metric], df[[\"epoch\", metric]], on=\"epoch\", how=\"outer\")\n",
        "                        metrics_dict[metric].rename(columns={metric: model_dir}, inplace=True)\n",
        "\n",
        "    # Save to Excel\n",
        "    print(f\"\\n⌛️ Saving compiled metrics to {output_file}...\")\n",
        "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
        "        for metric, df in metrics_dict.items():\n",
        "            if not df.empty:\n",
        "                df.to_excel(writer, sheet_name=metric, index=False)\n",
        "\n",
        "    print(f\"\\n✅ Training metrics compiled and saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Runner Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compile_evaluation_metrics(evaluation_dir, results_output_file)\n",
        "compile_training_metrics(training_dir, training_output_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
