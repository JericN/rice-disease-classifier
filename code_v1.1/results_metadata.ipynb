{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install --quiet datasets"
      ],
      "metadata": {
        "id": "xLIn3MfMwTDs",
        "outputId": "96e338fc-4ff9-44e1-a3a9-030f7e9ff1d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BdjY1hRRteAI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from transformers import AutoModelForImageClassification, ViTHybridForImageClassification\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UDJgWkc7x6m1",
        "outputId": "6cde750a-114f-401f-9b89-78af8b5c30c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_param_info(model_names, output_file):\n",
        "    model_data = []\n",
        "\n",
        "    for model_name in model_names:\n",
        "        print(f\"\\nüîç Model: {model_name}\")\n",
        "\n",
        "        # Load model based on type\n",
        "        if \"hybrid\" in model_name:\n",
        "            model = ViTHybridForImageClassification.from_pretrained(model_name, num_labels=8, ignore_mismatched_sizes=True)\n",
        "        else:\n",
        "            model = AutoModelForImageClassification.from_pretrained(model_name, num_labels=8, ignore_mismatched_sizes=True)\n",
        "\n",
        "        # Identify trainable parameters\n",
        "        for name, param in model.named_parameters():\n",
        "            param.requires_grad = name.startswith(('classifier', 'distillation_classifier'))\n",
        "\n",
        "        # Count parameters\n",
        "        num_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"Total Parameters: {num_params:,} | Trainable: {trainable_params:,}\")\n",
        "\n",
        "        # Append to list\n",
        "        model_data.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Total Parameters\": num_params,\n",
        "            \"Trainable Parameters\": trainable_params,\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame and save to Excel\n",
        "    df_models = pd.DataFrame(model_data)\n",
        "    return df_models\n",
        "\n",
        "def get_dataset_info(dataset_path):\n",
        "    dataset = load_dataset(dataset_path)\n",
        "\n",
        "    # Get label names from the dataset metadata\n",
        "    label_names = dataset[\"train\"].features[\"label\"].names  # Assuming all splits have the same labels\n",
        "\n",
        "    class_data = []\n",
        "\n",
        "    for split in dataset.keys():\n",
        "        df = dataset[split].to_pandas()\n",
        "        class_counts = df[\"label\"].value_counts().reset_index()\n",
        "        class_counts.columns = [\"Class Index\", \"Count\"]\n",
        "        class_counts[\"Class\"] = class_counts[\"Class Index\"].map(lambda x: label_names[x])  # Map index to label\n",
        "        class_counts[\"Split\"] = split\n",
        "        class_counts = class_counts[[\"Class\", \"Class Index\", \"Count\", \"Split\"]]  # Reorder columns\n",
        "        class_data.append(class_counts)\n",
        "\n",
        "    # Merge all splits into one DataFrame\n",
        "    df_dataset = pd.concat(class_data, ignore_index=True)\n",
        "    return df_dataset"
      ],
      "metadata": {
        "id": "lS5q3TIOtwnq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = [\n",
        "    \"microsoft/resnet-152\",\n",
        "    \"facebook/convnextv2-base-1k-224\",\n",
        "    \"google/vit-hybrid-base-bit-384\",\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    \"microsoft/swin-base-patch4-window7-224\",\n",
        "    \"facebook/deit-base-patch16-224\",\n",
        "    \"facebook/dinov2-base\",\n",
        "    \"apple/mobilevit-small\",\n",
        "    \"google/efficientnet-b0\",\n",
        "    \"facebook/convnextv2-femto-1k-224\",\n",
        "    \"WinKawaks/vit-tiny-patch16-224\",\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
        "    \"facebook/deit-tiny-patch16-224\"\n",
        "]\n",
        "dataset_path = 'cvmil/rice-leaf-disease-augmented-v4'\n",
        "output_file = \"/content/drive/Shareddrives/CS198-Drones/[v4] Results/model_and_dataset_info.xlsx\"\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "# Get model and dataset info\n",
        "df_models = get_model_param_info(model_list, output_file)\n",
        "df_dataset = get_dataset_info(dataset_path)\n",
        "\n",
        "# Save both tables to the same Excel file\n",
        "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "    df_models.to_excel(writer, sheet_name=\"Model Parameters\", index=False)\n",
        "    df_dataset.to_excel(writer, sheet_name=\"Dataset Info\", index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Results saved to {output_file}\")"
      ],
      "metadata": {
        "id": "VfPtKbvzxyg9",
        "outputId": "4a28d806-7e0e-4dd1-b04f-cb028bdce548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Model: microsoft/resnet-152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-152 and are newly initialized because the shapes did not match:\n",
            "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 58,160,200 | Trainable: 16,392\n",
            "\n",
            "üîç Model: facebook/convnextv2-base-1k-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ConvNextV2ForImageClassification were not initialized from the model checkpoint at facebook/convnextv2-base-1k-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([8, 1024]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 87,701,000 | Trainable: 8,200\n",
            "\n",
            "üîç Model: google/vit-hybrid-base-bit-384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTHybridForImageClassification were not initialized from the model checkpoint at google/vit-hybrid-base-bit-384 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 98,188,104 | Trainable: 6,152\n",
            "\n",
            "üîç Model: google/vit-base-patch16-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 85,804,808 | Trainable: 6,152\n",
            "\n",
            "üîç Model: microsoft/swin-base-patch4-window7-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([8, 1024]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 86,751,424 | Trainable: 8,200\n",
            "\n",
            "üîç Model: facebook/deit-base-patch16-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 85,804,808 | Trainable: 6,152\n",
            "\n",
            "üîç Model: facebook/dinov2-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 86,592,776 | Trainable: 12,296\n",
            "\n",
            "üîç Model: apple/mobilevit-small\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of MobileViTForImageClassification were not initialized from the model checkpoint at apple/mobilevit-small and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 640]) in the checkpoint and torch.Size([8, 640]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 4,942,760 | Trainable: 5,128\n",
            "\n",
            "üîç Model: google/efficientnet-b0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of EfficientNetForImageClassification were not initialized from the model checkpoint at google/efficientnet-b0 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 1280]) in the checkpoint and torch.Size([8, 1280]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 4,017,796 | Trainable: 10,248\n",
            "\n",
            "üîç Model: facebook/convnextv2-femto-1k-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ConvNextV2ForImageClassification were not initialized from the model checkpoint at facebook/convnextv2-femto-1k-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([8, 384]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 4,851,320 | Trainable: 3,080\n",
            "\n",
            "üîç Model: WinKawaks/vit-tiny-patch16-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([8, 192]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 5,525,960 | Trainable: 1,544\n",
            "\n",
            "üîç Model: microsoft/swin-tiny-patch4-window7-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 27,525,506 | Trainable: 6,152\n",
            "\n",
            "üîç Model: facebook/deit-tiny-patch16-224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at facebook/deit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([8, 192]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 5,525,960 | Trainable: 1,544\n",
            "\n",
            "‚úÖ Results saved to /content/drive/Shareddrives/CS198-Drones/[v4] Results/model_and_dataset_info.xlsx\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}